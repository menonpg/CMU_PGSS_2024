# Lecture 2 Notes


### Summary of Today's Lecture:
**Introduction to Lightning.AI**:
- Overview of how to set up a one-click studio using Lightning.AI.
- Discussion on preinstalled components for development and international local language models using Ollama.

**Ollama Setup on Lightning.AI:**

**Terminal Commands for setting up Ollama:**
- curl -fsSL https://ollama.com/install.sh | sh
- ollama pull llava-phi3
- ollama pull llama3

**Retrieval-Augmented Generation (RAG):**

**Explanation of RAG and its importance.**
- Steps to set up Ollama and create a Streamlit app for performing RAG on documents.
- Practical example of performing RAG on images using a vision language model (llava-phi3) to identify the number and types of animals in an image, with the output formatted as JSON.
- Using visual question answering on llava-phi3 from the Ollama terminal command line application.
- Performing RAG on llama3 with a Streamlit app.sing a vision language model to identify the number and types of animals in an image, with the output formatted as JSON.

### Review of Previous Class Setup Steps:
**Integrated Development Environments (IDEs)**:

**Cloud-based IDEs**:
- Code repositories and GitHub.
- Using Google Colab and Drive for Python development.
- Posit Cloud (R environment).
- Lightning.AI for AI model development.

**Local-based IDEs**:
- Visual Studio Code with plugins like Copilot, Jupyter, and Python.
- Terminal for cloning repositories and syncing.
- What is .gitignore and how to use .gitignore lists
- Anaconda Python package manager.

**Generating and Running Python Code**:
- Basics of writing and executing Python scripts on the terminal in addition to ipynb notebooks
- Practical setup and configuration for local and cloud environments.
